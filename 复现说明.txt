PAN_V2致力于PAN网络需要两阶段训练的问题:

1. 基础分支使用的是ResNet50,其结构为几个前端卷积层+4个ResBlock, 而每个ResBlock是由几个残差块组成的,且网络由ImageNet参数初始化;
2. 对齐分支使用的是ResNet50的后两个ResBlock构成,且和基础分支共用这两个ResBlock;
3. 仿射变换网络使用的是几个卷积层+几个BN层+AvgPool组成,主要是避免残差块搭建麻烦,且没有进行初始化;
4. 论文中提到的将theta初始化,本算法也没有使用;
5. 网络直接训练,不是分阶段,即一个传入一个batch数据后先走基础分支,然后从基础分支的Res5输出并将结果传入仿射网络得到仿射变换参数thets,对基础分支的
   Res2的结果进行仿射变换,得到的结果经过对齐分支得到第二个输出.且处理对齐分支时也不固定基础分支;
6. 网络的loss由基本分支和对齐分支两部分组成,各自占据一定权重;
7. 推断阶段中,将输出1的基础特征和输出2的对齐特征进行融合作为最终行人特征,而融合方式中,我们设置了5种,分别是only_base, only_affine, concat, avg, max,
   分别代表只使用基础分支的特征,只使用对齐分支的特征,将两个分支的特征按channel连接,将两个分支的特征按channel取平均,将两个分支的特征按channel取最大.


但最终结果mAP大约比作者报告结果低十几个百分点,但是不需要两阶段训练这个是一大优势.在Market1501上Not Normalize Feature的结果如下:
训练150epoch, lr在76epoch时decay===>
===============================================================================
            |              result            |         rerank result          |
            |  mAP  | rank1 | rank5 | rank10 |  mAP  | rank1 | rank5 | rank10 |            
only base   |40.91% |60.54% |82.01% | 88.63% |53.78% |64.73% |80.20% | 85.45% |
max         |45.54% |66.72% |84.59% | 90.02% |59.21% |72.03% |82.78% | 87.00% |    
concat      |52.40% |72.18% |87.50% | 92.10% |65.58% |75.50% |86.28% | 89.31% |
avg         |50.53% |70.69% |86.52% | 91.57% |63.44% |72.86% |85.39% | 89.61% |
only affine |45.37% |66.42% |84.80% | 90.23% |58.60% |69.54% |82.96% | 87.29% |
===============================================================================
我们可以得出以下结论:
concat的结果最高,和作者结论一致;且进一步实验可以NF比NNF效果会高大约4个点.


作者报告结果如下:  market1501
===============================================================================
            |              result            |         rerank result          |  
            |  mAP  | rank1 | rank5 | rank20 |  mAP  | rank1 | rank5 | rank20 |          
only base   |59.14% |80.17% |91.69% | 96.59% |       |       |       |        |    
concat      |58.27% |79.01% |90.86% | 96.14% |       |       |       |        |
only affine |63.35% |82.81% |93.53% | 97.06% |76.56% |85.78% |       |        |
===============================================================================











